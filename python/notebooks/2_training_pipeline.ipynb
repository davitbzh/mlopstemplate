{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <span style=\"font-width:bold; font-size: 3rem; color:#1EB182;\"> **Hopsworks Feature Store** </span><span style=\"font-width:bold; font-size: 3rem; color:#333;\">- Part 02: Training Pipeline</span>\n",
    "\n",
    "<span style=\"font-width:bold; font-size: 1.4rem;\">This notebook explains how to read from a feature group, create training dataset within the feature store, train a model and save it to model registry.</span>\n",
    "\n",
    "## üóíÔ∏è This notebook is divided into the following sections:\n",
    "\n",
    "1. Fetch Feature Groups.\n",
    "2. Define Transformation functions.\n",
    "3. Create Feature Views.\n",
    "4. Create Training Dataset with training, validation and test splits.\n",
    "5. Train the model.\n",
    "6. Register model in Hopsworks Model Registry.\n",
    "7. Create the Deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'> üìù Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-01 13:01:41,231 INFO: generated new fontManager\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "import seaborn as sns\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Mute warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üì° Connecting to Hopsworks Feature Store </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "\n",
      "Logged in to project, explore it here https://030275a0-8fa6-11ee-983c-3f72893426f0.cloud.hopsworks.ai/p/119\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "import hopsworks\n",
    "\n",
    "project = hopsworks.login()\n",
    "\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üî™ Feature Selection </span>\n",
    "\n",
    "You will start by selecting all the features you want to include for model training/inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature groups.\n",
    "trans_fg = fs.get_feature_group('transactions', version=1)\n",
    "labels_fg = fs.get_feature_group('fraud_labels', version=1)\n",
    "profile_fg = fs.get_feature_group('profile', version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = labels_fg.select([\"fraud_label\"])\\\n",
    "                 .join(trans_fg.select([\"amount\", \"loc_delta_t_minus_1\", \"time_delta_t_minus_1\", \n",
    "                                        \"days_until_card_expires\", \"age_at_transaction\", \"number_of_transactions_daily\",\n",
    "                                        \"datetime\", \"latitude\", \"longitude\"]))\\\n",
    "                 .join(profile_fg.select([\"cc_expiration_date\", \"birthdate\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## uncomment this if you would like to view query results\n",
    "#query.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that you computed the features in `transactions_fraud_online_fg`. If you had created multiple feature groups with identical schema for different window lengths, and wanted to include them in the join you would need to include a prefix argument in the join to avoid feature name clash. See the [documentation](https://docs.hopsworks.ai/feature-store-api/latest/generated/api/query_api/#join) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ff5f27;\"> ü§ñ Transformation Functions </span>\n",
    "\n",
    "\n",
    "You will preprocess our data using *min-max scaling* on numerical features and *label encoding* on categorical features. To do this you simply define a mapping between our features and transformation functions. This ensures that transformation functions such as *min-max scaling* are fitted only on the training data (and not the validation/test data), which ensures that there is no data leakage.\n",
    "\n",
    "# Load the transformation functions.\n",
    "label_encoder = fs.get_transformation_function(name=\"label_encoder\")\n",
    "\n",
    "# Map features to transformation functions.\n",
    "transformation_functions = {\n",
    "    \"country\": label_encoder,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> ‚öôÔ∏è Feature View Creation </span>\n",
    "\n",
    "The Feature Views allows schema in form of a query with filters, define a model target feature/label and additional transformation functions.\n",
    "In order to create or get a Feature View you may use `fs.get_or_create_feature_view()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature view created successfully, explore it at \n",
      "https://030275a0-8fa6-11ee-983c-3f72893426f0.cloud.hopsworks.ai/p/119/fs/67/fv/transactions_fraud_online_fv/version/1\n"
     ]
    }
   ],
   "source": [
    "feature_view = fs.get_or_create_feature_view(\n",
    "    name='transactions_fraud_online_fv',\n",
    "    version=1,\n",
    "    query=query,\n",
    "    labels=[\"fraud_label\"],\n",
    "    inference_helper_columns=[\"datetime\", \"latitude\", \"longitude\", \"cc_expiration_date\", \"birthdate\"]\n",
    "    #transformation_functions=transformation_functions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_view = fs.get_feature_view(\n",
    "    name='transactions_fraud_online_fv',\n",
    "    version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from mlopstemplate.features import transactions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<hsfs.core.job.Job at 0x7fa0f0d427a0>, None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: this snippet needs to be in prediction application\n",
    "\n",
    "# new input \n",
    "app_input = {\n",
    " 'tid': 'e3c3a3f1610ff99059f2455d14b4e748',\n",
    " 'datetime': '2023-06-04 10:48:32',\n",
    " 'cc_num': 4020740456971257,\n",
    " 'category': 'Grocery',\n",
    " 'amount': 83.99,\n",
    " 'latitude': 36.02506,\n",
    " 'longitude': -86.77917,\n",
    " 'city': 'Brentwood Estates',\n",
    " 'country': 'US'\n",
    "}\n",
    "\n",
    "# get application inputs\n",
    "tid = app_input['tid']\n",
    "cc_num = app_input['cc_num']\n",
    "tr_date = datetime.strptime(app_input['datetime'], '%Y-%m-%d %H:%M:%S')\n",
    "latitude = app_input['latitude']\n",
    "longitude = app_input['longitude']\n",
    "country = app_input['country']\n",
    "amount = app_input['amount']\n",
    "\n",
    "# get helper columns\n",
    "helpers_df = feature_view.get_inference_helper({\"cc_num\": cc_num}, return_type=\"pandas\")\n",
    "\n",
    "# assemble feature vector\n",
    "feature_vector = feature_view.get_feature_vector({\"cc_num\": cc_num}, return_type=\"pandas\")\n",
    "feature_vector[\"amount\"] = amount\n",
    "loc_delta_t_minus_1 = transactions.haversine(latitude, longitude, helpers_df.longitude, helpers_df.latitude)\n",
    "time_delta_t_minus_1 = transactions.time_delta_t_minus_1(tr_date, helpers_df.datetime)\n",
    "feature_vector[\"days_until_card_expires\"] = transactions.expiry_days(helpers_df.cc_expiration_date, tr_date)\n",
    "feature_vector[\"age_at_transaction\"] = transactions.card_owner_age(helpers_df.birthdate, tr_date)\n",
    "\n",
    "# if new date increment\n",
    "if helpers_df[\"datetime\"].map(lambda x: transactions.is_new_calendar_date(x)).values[0]: \n",
    "    feature_vector[\"number_of_transactions_daily\"] = feature_vector.number_of_transactions_daily.map(lambda x: x + 1)\n",
    "else: \n",
    "    feature_vector[\"number_of_transactions_daily\"] = 1\n",
    "    \n",
    "\n",
    "# define insert feature group\n",
    "fg_df = feature_vector.copy()\n",
    "\n",
    "fg_df[\"tid\"] = tid\n",
    "fg_df[\"cc_num\"] = cc_num\n",
    "fg_df[\"datetime\"] = tr_date\n",
    "fg_df[\"month\"] = transactions.get_year_month(fg_df.datetime)\n",
    "fg_df[\"year_month_day\"] = str(tr_date.year) + \"-\" + str(tr_date.month) + \"-\" + str(tr_date.day)\n",
    "fg_df[\"country\"] = country\n",
    "fg_df[\"latitude\"] = latitude\n",
    "fg_df[\"longitude\"] = longitude\n",
    "\n",
    "# insert into the feature group\n",
    "trans_fg = fs.get_feature_group('transactions', version=1)\n",
    "trans_fg.multi_part_insert(fg_df, validation_options={\"run_validation\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Bellow you define model and then prediction application\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üèãÔ∏è Training Dataset Creation</span>\n",
    "\n",
    "In Hopsworks training data is a query where the projection (set of features) is determined by the parent FeatureView with an optional snapshot on disk of the data returned by the query.\n",
    "\n",
    "**Training Dataset  may contain splits such as:** \n",
    "* Training set - the subset of training data used to train a model.\n",
    "* Validation set - the subset of training data used to evaluate hparams when training a model\n",
    "* Test set - the holdout subset of training data used to evaluate a mode\n",
    "\n",
    "Dataset with train/test splits can be created using `fs.create_train_test_split()` method.\n",
    "Dataset with train/valid/test splits can be created using `fs.create_train_validation_test_split()` method.\n",
    "\n",
    "**You can use event time filters like `train_start`, `train_end`, `valid_start`, `valid_end`... Values can be either in unix, string and datetime format.** \n",
    "\n",
    "**Or, use `validation_size` and `test_size` parameters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split date with percentage \n",
    "import datetime\n",
    "pdf = labels_fg.read()\n",
    "def split_dfs(df): \n",
    "    #df.datetime =  df.datetime.map(lambda x: datetime.datetime.fromtimestamp(x//1000))\n",
    "    df = df.sort_values(by='datetime') \n",
    "    trainvals = df[:int(len(df)*0.8)] \n",
    "    testvals = df[int(len(df)*0.8):] \n",
    "    return {'train_start': min(trainvals.datetime).date(), 'train_end': max(trainvals.datetime).date(), 'test_start': min(testvals.datetime).date(), 'test_end': max(testvals.datetime).date()}\n",
    "split_dict = split_dfs(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training/Test splits, datasets creation. Using timerange arguments.\n",
    "td_version, td_job = feature_view.create_train_test_split(\n",
    "    train_start=split_dict['train_start'],\n",
    "    train_end=split_dict['train_end'],\n",
    "    test_start=split_dict['test_start'],\n",
    "    test_end=split_dict['test_end'],\n",
    "    data_format = \"csv\",\n",
    "    coalesce = True,\n",
    "    write_options = {'wait_for_job': True},\n",
    "    )\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = feature_view.get_train_test_split(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.sort_values(\"datetime\")\n",
    "y_train = y_train.reindex(X_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.sort_values(\"datetime\")\n",
    "y_test = y_test.reindex(X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = X_test.cc_num.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop([\"cc_num\", \"datetime\"], axis=1, inplace=True)\n",
    "X_test.drop([\"cc_num\",\"datetime\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the distribution is extremely skewed, which is natural considering that fraudulent transactions make up a tiny part of all transactions. Thus you should somehow address the class imbalance. There are many approaches for this, such as weighting the loss function, over- or undersampling, creating synthetic data, or modifying the decision threshold. In this example, you will use the simplest method which is to just supply a class weight parameter to our learning algorithm. The class weight will affect how much importance is attached to each class, which in our case means that higher importance will be placed on positive (fraudulent) samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üß¨ Modeling</span>\n",
    "\n",
    "Next you will train a model. Here, you set larger class weight for the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf = xgb.XGBClassifier()\n",
    "\n",
    "clf.fit(X_train.values, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Predictions\n",
    "y_pred_train = clf.predict(X_train.values)\n",
    "\n",
    "# Test Predictions\n",
    "y_pred_test = clf.predict(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute f1 score\n",
    "metrics = {\"f1_score\": f1_score(y_test, y_pred_test, average='macro')}\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = confusion_matrix(y_test, y_pred_test, labels=[False, True])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cm = pd.DataFrame(results, ['True Normal', 'True Fraud'],['Pred Normal', 'Pred Fraud'])\n",
    "\n",
    "cm = sns.heatmap(df_cm, annot=True)\n",
    "\n",
    "fig = cm.get_figure() \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <span style=\"color:#ff5f27;\">‚öôÔ∏è Model Schema</span>\n",
    "\n",
    "The model needs to be set up with a [Model Schema](https://docs.hopsworks.ai/3.0/user_guides/mlops/registry/model_schema/), which describes the inputs and outputs for a model.\n",
    "\n",
    "A Model Schema can be automatically generated from training examples, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hsml.schema import Schema\n",
    "from hsml.model_schema import ModelSchema\n",
    "\n",
    "input_schema = Schema(X_train.values)\n",
    "output_schema = Schema(y_train)\n",
    "model_schema = ModelSchema(input_schema=input_schema, output_schema=output_schema)\n",
    "\n",
    "model_schema.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\">üìù Register model</span>\n",
    "\n",
    "One of the features in Hopsworks is the model registry. This is where we can store different versions of models and compare their performance. Models from the registry can then be served as API endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'fraud_online_model' directory will be saved to the model registry\n",
    "model_dir=\"fraud_online_model\"\n",
    "if os.path.isdir(model_dir) == False:\n",
    "    os.mkdir(model_dir)\n",
    "\n",
    "joblib.dump(clf, model_dir + '/xgboost_fraud_online_model.pkl')\n",
    "\n",
    "fig.savefig(model_dir + \"/confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr = project.get_model_registry()\n",
    "\n",
    "fraud_model = mr.python.create_model(\n",
    "    name=\"xgboost_fraud_online_model\", \n",
    "    metrics=metrics,\n",
    "    model_schema=model_schema,\n",
    "    input_example=[test_sample], # for testing deployments\n",
    "    description=\"Fraud Online Predictor\")\n",
    "\n",
    "fraud_model.save(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <a class=\"anchor\" id=\"1.5_bullet\" style=\"color:#ff5f27\"> üöÄ Model Deployment</a>\n",
    "\n",
    "\n",
    "### About Model Serving\n",
    "Models can be served via KFServing or \"default\" serving, which means a Docker container exposing a Flask server. For KFServing models, or models written in Tensorflow, you do not need to write a prediction file (see the section below). However, for sklearn models using default serving, you do need to proceed to write a prediction file.\n",
    "\n",
    "In order to use KFServing, you must have Kubernetes installed and enabled on your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ff5f27;\">üìé Predictor script for Python models</span>\n",
    "\n",
    "\n",
    "Scikit-learn and XGBoost models are deployed as Python models, in which case you need to provide a **Predict** class that implements the **predict** method. The **predict()** method invokes the model on the inputs and returns the prediction as a list.\n",
    "\n",
    "The **init()** method is run when the predictor is loaded into memory, loading the model from the local directory it is materialized to, *ARTIFACT_FILES_PATH*.\n",
    "\n",
    "The directive \"%%writefile\" writes out the cell before to the given Python file. We will use the **predict_example.py** file to create a deployment for our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile predict_example.py\n",
    "import os\n",
    "import numpy as np\n",
    "import hsfs\n",
    "import joblib\n",
    "\n",
    "\n",
    "class Predict(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" Initializes the serving state, reads a trained model\"\"\"        \n",
    "        # get feature store handle\n",
    "        fs_conn = hsfs.connection()\n",
    "        self.fs = fs_conn.get_feature_store()\n",
    "        \n",
    "        # get feature view\n",
    "        self.fv = self.fs.get_feature_view(\"transactions_fraud_online_fv\", 1)\n",
    "        \n",
    "        # initialize serving\n",
    "        self.fv.init_serving(1)\n",
    "\n",
    "        # load the trained model\n",
    "        self.model = joblib.load(os.environ[\"ARTIFACT_FILES_PATH\"] + \"/xgboost_fraud_online_model.pkl\")\n",
    "        print(\"Initialization Complete\")\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        \"\"\" Serves a prediction request usign a trained model\"\"\"\n",
    "        feature_vector = self.fv.get_feature_vector({\"cc_num\": inputs[0][0]})\n",
    "        indexes_to_remove = [0,1]\n",
    "        feature_vector = [i for j, i in enumerate(feature_vector) if j not in indexes_to_remove]\n",
    "        \n",
    "        return self.model.predict(np.asarray(feature_vector).reshape(1, -1)).tolist() # Numpy Arrays are not JSON serializable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wonder why we use the path Models/fraud_tutorial_model/1/model.pkl, it is useful to know that the Data Sets tab in the Hopsworks UI lets you browse among the different files in the project. Registered models will be found underneath the Models directory. Since you saved you model with the name fraud_tutorial_model, that's the directory you should look in. 1 is just the version of the model you want to deploy.\n",
    "\n",
    "This script needs to be put into a known location in the Hopsworks file system. Let's call the file predict_example.py and put it in the Models directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_api = project.get_dataset_api()\n",
    "\n",
    "uploaded_file_path = dataset_api.upload(\"predict_example.py\", \"Models\", overwrite=True)\n",
    "predictor_script_path = os.path.join(\"/Projects\", project.name, uploaded_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the deployment\n",
    "Here, you fetch the model you want from the model registry and define a configuration for the deployment. For the configuration, you need to specify the serving type (default or KFserving)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment = fraud_model.deploy(\n",
    "    name=\"fraudonlinemodeldeployment\",\n",
    "    script_file=predictor_script_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Deployment: \" + deployment.name)\n",
    "deployment.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Deployment is warming up...\")\n",
    "time.sleep(45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The deployment has now been registered. However, to start it you need to run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "deployment.start(await_running=180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment.get_state().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to troubleshoot you can use `get_logs()` method\n",
    "deployment.get_logs(component='predictor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Deployment\n",
    "To stop the deployment you simply run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment.stop(await_stopped=180)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:#ff5f27;\">‚è≠Ô∏è **Next:** Part 03: Inference Pipeline</span>\n",
    "\n",
    "In the following notebook you will use your model for Serving Vector Inference.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/logicalclocks/hopsworks-tutorials/blob/master/fraud_online/3_fraud_online_inference_pipeline.ipynb)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
