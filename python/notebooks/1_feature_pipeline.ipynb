{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5487abb7",
   "metadata": {
    "tags": []
   },
   "source": [
    "This is the first part of the quick start series of tutorials about Hopsworks Feature Store. As part of this first module, you will work with data related to credit card transactions. \n",
    "The objective of this tutorial is to demonstrate how to work with the **Hopworks Feature Store**  for online data with a goal of training and deploying a model that can predict fraudulent transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a06bde1",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'> üìù Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5088f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hopsworks\n",
    "\n",
    "import great_expectations as ge\n",
    "from great_expectations.core import ExpectationSuite, ExpectationConfiguration\n",
    "\n",
    "from mlopstemplate.features import transactions, profile\n",
    "from mlopstemplate.synthetic_data import synthetic_data\n",
    "from mlopstemplate.synthetic_data.data_sources import get_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a22611",
   "metadata": {},
   "source": [
    "First of all you will load the data and do some feature engineering on it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41a3e7f",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üíΩ Loading the Data </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595d30b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data from the source\n",
    "trans_df, labels_df, profiles_df = get_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65834e3b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ba68d7",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üõ†Ô∏è Feature Engineering </span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a710ae96",
   "metadata": {},
   "source": [
    "Now you are ready to start by computing the distance between consecutive transactions, lets call it `loc_delta`.\n",
    "Here you will use the [Haversine distance](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.haversine_distances.html?highlight=haversine#sklearn.metrics.pairwise.haversine_distances) to quantify the distance between two longitude and latitude coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4a4f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute profile features\n",
    "# select final features\n",
    "profiles_df = profile.select_features(profiles_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c40905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute transaction features\n",
    "trans_df = trans_df.sort_values(by=['cc_num', 'datetime'])\n",
    "trans_df[\"prev_transaction_date\"] = trans_df.groupby(\"cc_num\").apply(lambda x: x[\"datetime\"].shift(1)).reset_index(level=0, drop=True)\n",
    "trans_df[\"prev_longitude\"] = trans_df.groupby(\"cc_num\").apply(lambda x: x[\"longitude\"].shift(1)).reset_index(level=0, drop=True)\n",
    "trans_df[\"prev_latitude\"] = trans_df.groupby(\"cc_num\").apply(lambda x: x[\"latitude\"].shift(1)).reset_index(level=0, drop=True)\n",
    "trans_df = trans_df.dropna()\n",
    "\n",
    "# compute previous location of the transaction\n",
    "trans_df[\"loc_delta_t_minus_1\"] = transactions.haversine(trans_df.longitude, trans_df.latitude, trans_df.prev_longitude, trans_df.prev_latitude)\n",
    "\n",
    "# Computes time difference between current and previous transaction\n",
    "trans_df[\"time_delta_t_minus_1\"] = transactions.time_delta_t_minus_1(trans_df.datetime, trans_df.prev_transaction_date)\n",
    "\n",
    "# compute number of transactions per day\n",
    "trans_df[\"year_month_day\"] = trans_df.datetime.map(lambda x: str(x.year) + \"-\" + str(x.month) + \"-\" + str(x.day))\n",
    "count_by_day = trans_df.groupby(['cc_num', 'year_month_day']).size().reset_index(name='number_of_transactions_daily')\n",
    "trans_df = trans_df.merge(count_by_day, on=[\"cc_num\", \"year_month_day\"], how=\"left\")\n",
    "\n",
    "# Compute year and month string from datetime column, for partition \n",
    "trans_df[\"month\"] = transactions.get_year_month(trans_df.datetime)\n",
    "\n",
    "# compute on demand features\n",
    "# customer's age at transaction\n",
    "trans_df = trans_df.merge(profiles_df[[\"cc_num\", \"birthdate\", \"cc_expiration_date\"]], on=[\"cc_num\"], how=\"left\")\n",
    "\n",
    "# age at transaction\n",
    "trans_df[\"age_at_transaction\"] = transactions.card_owner_age(trans_df.datetime, trans_df.birthdate)\n",
    "\n",
    "# days untill card expires at the time of transaction\n",
    "trans_df[\"days_until_card_expires\"] = transactions.expiry_days(trans_df.datetime, trans_df.cc_expiration_date)\n",
    "\n",
    "# select features\n",
    "trans_df = transactions.select_features(trans_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f974fec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels\n",
    "labels_df[\"month\"] = transactions.get_year_month(labels_df.datetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1f3583",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> Great Expectations </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cab3e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "ge_trans_df = ge.from_pandas(trans_df)\n",
    "\n",
    "expectation_suite_trans = ge_trans_df.get_expectation_suite()\n",
    "expectation_suite_trans.expectation_suite_name = \"transaction_suite\"\n",
    "print(expectation_suite_trans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5273006a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for errors which could lead to technical issues\n",
    "expectation_suite_trans.add_expectation(\n",
    "    ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_values_to_be_unique\",\n",
    "        kwargs={\"column\":\"tid\", \"result_format\":\"COMPLETE\"}\n",
    "    )\n",
    ")\n",
    "\n",
    "# Assess data correctness\n",
    "expectation_suite_trans.add_expectation(\n",
    "    ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_min_to_be_between\",\n",
    "        kwargs={\n",
    "            \"column\":\"amount\",\n",
    "            \"min_value\": 0\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "expectation_suite_trans.add_expectation(\n",
    "    ExpectationConfiguration(\n",
    "        expectation_type=\"expect_column_min_to_be_between\",\n",
    "        kwargs={\n",
    "            \"column\":\"age_at_transaction\",\n",
    "            \"min_value\": 17,\n",
    "            \"max_value\": 101\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "ge_trans_df = ge.from_pandas(trans_df, expectation_suite=expectation_suite_trans)\n",
    "\n",
    "validation_report_trans = ge_trans_df.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c651e247",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_report_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20af7940",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate an Expectation Suite from your data using Great Expectations Profiler\n",
    "#For complex DataFrames, Great Expectations offers a profiler which generates a basic expectation suite tailored to your data. You can then use this suite as you would any other expectation suite with Hopsworks.\n",
    "\n",
    "# ignore deprecation warnings\n",
    "expectation_suite_profiled, validation_report = ge_trans_df.profile(profiler=ge.profile.BasicSuiteBuilderProfiler)\n",
    "\n",
    "print(f\"The suite contains {len(expectation_suite_profiled['expectations'])} expectations for {len(trans_df.columns.values)} columns. See sample below\\n\" + ge_trans_df.get_expectation_suite().__repr__()[:455])\n",
    "\n",
    "#Note that you cannot register the report generated by the profiler as the suite was not registered with Hopsworks before the validation was run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c39ebb",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üì° Connecting to Hopsworks Feature Store </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9dbd66",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ff5f27;\"> ü™Ñ Creating Feature Groups </span>\n",
    "\n",
    "A [feature group](https://docs.hopsworks.ai/3.0/concepts/fs/feature_group/fg_overview/) can be seen as a collection of conceptually related features. In this case, you will create a feature group for the transaction data and a feature group for the windowed aggregations on the transaction data. Both will have `cc_num` as primary key, which will allow you to join them when creating a dataset in the next tutorial.\n",
    "\n",
    "Feature groups can also be used to define a namespace for features. For instance, in a real-life setting you would likely want to experiment with different window lengths. In that case, you can create feature groups with identical schema for each window length. \n",
    "\n",
    "Before you can create a feature group you need to connect to Hopsworks feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cc2d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hopsworks\n",
    "\n",
    "project = hopsworks.login()\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dad159b",
   "metadata": {},
   "source": [
    "To create a feature group you need to give it a name and specify a primary key. It is also good to provide a description of the contents of the feature group and a version number, if it is not defined it will automatically be incremented to `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea75a32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get or create feature group\n",
    "trans_fg = fs.get_or_create_feature_group(\n",
    "    name=\"transactions\",\n",
    "    version=1,\n",
    "    description=\"Transaction data\",\n",
    "    primary_key=['cc_num'],\n",
    "    event_time='datetime',\n",
    "    partition_key=['month'],\n",
    "    stream=True,\n",
    "    online_enabled=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7549a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.Setup Automatic Validation On Insert and upload a DataFrame\n",
    "# Register the expectation suite corresponding to a Feature Group with the backend\n",
    "\n",
    "# The \"ALWAYS\" ingestion policy inserts data even when validation fails, \n",
    "# ideal to avoid data loss and rapid prototyping\n",
    "\n",
    "trans_fg.save_expectation_suite(expectation_suite_trans, validation_ingestion_policy=\"ALWAYS\")\n",
    "\n",
    "#Once the suite is registered in the backend, data validation will run on every insert without additional boilerplate. The suite is retrieved from the backend, used to validate the DataFrame and the resulting validation report uploaded. Depending on the ingestion policy and validation success, data are subsequently inserted in the Feature Group. The example below illustrate the \"ALWAYS\" use case where insertion is performed despite a validation failure.\n",
    "\n",
    "# materialize feature data in to the feature group\n",
    "trans_fg.insert(trans_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fea6c7",
   "metadata": {},
   "source": [
    "Here you have also set `online_enabled=True`, which enables low latency access to the data. A full list of arguments can be found in the [documentation](https://docs.hopsworks.ai/feature-store-api/latest/generated/api/feature_store_api/#create_feature_group)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a974fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_descriptions = [\n",
    "    {\"name\": \"tid\", \"description\": \"Transaction id\"},\n",
    "    {\"name\": \"datetime\", \"description\": \"Transaction time\"},\n",
    "    {\"name\": \"cc_num\", \"description\": \"Number of the credit card performing the transaction\"},\n",
    "    {\"name\": \"amount\", \"description\": \"Month of transaction\"},\n",
    "    {\"name\": \"year_month_day\", \"description\": \"Day of transaction\"},\n",
    "    {\"name\": \"latitude\", \"description\": \"latitude of the place where merchant or ATM is located\"},\n",
    "    {\"name\": \"longitude\", \"description\": \"longitude of the place where merchant or ATM is located\"},\n",
    "    {\"name\": \"country\", \"description\": \"Country in which the transaction was made\"},\n",
    "    {\"name\": \"loc_delta_t_minus_1\", \"description\": \"Location of previous transaction\"},\n",
    "    {\"name\": \"time_delta_t_minus_1\", \"description\": \"Time of previous transaction\"},    \n",
    "    {\"name\": \"month\", \"description\": \"Month of the transaction\"},    \n",
    "    {\"name\": \"age_at_transaction\", \"description\": \"Age of card holder at the time of transaction\"},\n",
    "    {\"name\": \"days_until_card_expires\", \"description\": \"Days left till card expires\"},\n",
    "    {\"name\": \"number_of_transactions_daily\", \"description\": \"Number of transactions per cc daily\"},\n",
    "]\n",
    "\n",
    "for desc in feature_descriptions: \n",
    "    trans_fg.update_feature_description(desc[\"name\"], desc[\"description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db88987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get or create feature group\n",
    "labels_fg = fs.get_or_create_feature_group(\n",
    "    name=\"fraud_labels\",\n",
    "    version=1,\n",
    "    description=\"Transaction data\",\n",
    "    primary_key=['cc_num'],\n",
    "    event_time='datetime',\n",
    "    partition_key=['month'],\n",
    "    stream=True,\n",
    "    online_enabled=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f22387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# materialize feature data in to the feature group\n",
    "labels_fg.insert(labels_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638b882b",
   "metadata": {},
   "source": [
    "You can move on and do the same thing for the profile and label feature groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8124946f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get or create feature group\n",
    "profile_fg = fs.get_or_create_feature_group(\n",
    "    name=\"profile\",\n",
    "    version=1,\n",
    "    description=\"Credit card holder demographic data\",\n",
    "    primary_key=[\"cc_num\"],\n",
    "    partition_key=[\"cc_provider\"],\n",
    "    stream=True,\n",
    "    online_enabled=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e88c342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# materialize feature data in to the feature group\n",
    "profile_fg.insert(profiles_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f90a6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_descriptions = [\n",
    "    {\"name\": \"cc_num\", \"description\": \"Number of the credit card performing the transaction\"},\n",
    "    {\"name\": \"cc_provider\", \"description\": \"Company name that issued the card\"},\n",
    "    {\"name\": \"cc_type\", \"description\": \"Type of the card, debit or credit\"},\n",
    "    {\"name\": \"cc_expiration_date\", \"description\": \"Date when this card expires\"},    \n",
    "    {\"name\": \"birthdate\", \"description\": \"Birth date\"},\n",
    "    {\"name\": \"country_of_residence\", \"description\": \"Country of residence of the card holder\"},    \n",
    "]\n",
    "\n",
    "for desc in feature_descriptions: \n",
    "    profile_fg.update_feature_description(desc[\"name\"], desc[\"description\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63acce9",
   "metadata": {},
   "source": [
    "Click on the hyperlink printed in the cell output above to inspect your feature group in the UI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec599503",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### <span style=\"color:#ff5f27;\"> üëì  Exploration</span>\n",
    "In the Hopsworks feature store, the metadata allows for multiple levels of explorations and review. Here you will explore a few of those capacities. \n",
    "\n",
    "### <span style=\"color:#ff5f27;\"> üîé Search</span>\n",
    "Using the search function in the UI, you can query any aspect of the feature groups, feature_view and training data that was previously created.\n",
    "\n",
    "### <span style=\"color:#ff5f27;\"> üìä Statistics</span>\n",
    "You can also enable statistics in one or all the feature groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492ea5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_fg = fs.get_or_create_feature_group(\"transactions\", version=1)\n",
    "trans_fg.statistics_config = {\n",
    "    \"enabled\": True,\n",
    "    \"histograms\": True,\n",
    "    \"correlations\": True\n",
    "}\n",
    "\n",
    "trans_fg.update_statistics_config()\n",
    "trans_fg.compute_statistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19080c6c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### ‚õìÔ∏è <b> Lineage </b> \n",
    "In all the feature groups and feature view you can look at the relation between each abstractions; what feature group created which training dataset and that is used in which model.\n",
    "This allows for a clear undestanding of the pipeline in relation to each element. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e1ddeae6eefc765c17da80d38ea59b893ab18c0c0904077a035ef84cfe367f83"
  },
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}